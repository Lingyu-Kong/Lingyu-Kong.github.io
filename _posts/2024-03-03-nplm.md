---
title: "Columns - Neural Probabilistic Language Model (1): RNN+LSTM+CE+NCE"
date: 2024-03-03
tags:
  - NLP
  - Loss Function
---

> 本篇是 Neural Probabilistic Language Model （神经概率语言模型）的第一篇。我通过梳理NPLM发展的过程，总结了NPLM的具有突出意义的研究如下：
> - [Yoshua Bengio开山之作](https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html)
> - [RNN模型 + 首次提出Sequence2Sequence问题的解决方案](https://arxiv.org/abs/1409.3215)
> - [LSTM模型](https://blog.xpgreat.com/file/lstm.pdf)
> - [Word2Vec: CBOW, Skip-gram](https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93)
> - [GloVe](https://aclanthology.org/D14-1162.pdf)
> - [Transformer（Self-Attention）](https://arxiv.org/abs/1706.03762)
> - BERT, GPT 等等（预训练+大模型时代）
> 
> 本文着重梳理NPLM早期思想，RNN+LSTM模型，以及早期的CrossEntropy和Noise Contrastive Estimation这两种Loss Function

在最初的Benjio的文章中，提出了NPLM的初级问题：给出上文$w=(w_1, w_2, ..., w_k)$和词典$V$，如何确定下一个单词$c$，即求
$$
P(c|(w_1, w_2, ..., w_k)), c\in V
$$

因为要模拟的是语言相关的概率分布模型，所以这类问题统一称为statistical language modeling

## 开山之作NPLM

Benjio首先假设上文$w$中重要的是最后n个单词，所以$w$被截断为统一的长度$(w_{k-n+1},...,w_k)$，然后使用一个神经网路模型来模拟：
$$
P_{\theta}(c|(w_1, w_2, ..., w_k)), c\in V
$$
具体的模型架构见[这篇介绍](https://zhuanlan.zhihu.com/p/21240807)，可以看到主要还是基于MLP模型进行的设计

[Paper Link](https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html)

## RNN模型

Benjio文章里使用的模型存在两个问题：
1. 需要截断上文以保证输入长度一致
2. 上文中处于不同位置的两个单词$w_i, w_j$在模型中并没有区分开其位置关系的不同

RNN模型的意义就在于解决了上面两点问题，并且提高了处理长文本输入的能力

[Paper Link](https://arxiv.org/abs/1409.3215)

> 这篇文章还第一次提出了Sequence to Sequence的架构，为后续的更加复杂的NLP问题的解决提供了基础

## LSTM模型

关于RNN和LSTM的模型的具体结构，[这篇文章](https://zhuanlan.zhihu.com/p/108276255)已经进行了介绍

## CrossEntropy

假设$batch\_size=N$，词典大小为$|V|$
$$
\text{CELoss}=-\frac{1}{N}\sum\limits_{i=1}^{N}\sum\limits_{v=1}^{|V|}y_v^{(i)}\log(p_v^{(i)})
$$
其中$p_v^{(i)}$是可以通过softmax层得到的：

## Noise Contrastive Estimation

CorssEntropy是多分类问题的经典Loss Function，但是用来做语言模型存在一个问题：词典大小$|V|$一般来讲非常大，在进行softmax计算的时候，由于每个数据点都需要涉及$|V|$次的指数运算（torch.exp()），指数运算本来就慢，还要做很多次，所以softmax层会很慢

那么CrossEntropy Loss究竟在学什么呢？对于单个数据点找出正确答案 $y_t=1$ ，然后看CELoss：
$$
\text{CELoss}=-\log\left(\frac{\exp(\text{logit}_t)}{\exp(\text{logit}_t)+\sum\limits_{v\neq t}^{|V|}\exp(\text{logit}_v)}\right)
$$

> 上面式子中的logit是网络在softmax层之前的输出，具体的含义分析见[我的另一篇文章](https://lingyu-kong.github.io/logit-logistic-regression/) 

由于上面的式子，优化的过程就是让$\text{logit}_t$变大，让错误答案的$\text{logit}_v$减小。并且由于softmax层包含了所有单词选项，所以所有的错误答案的$\text{logit}_v$都会纳入Loss计算。

但是如果我们只挑选一部分错误答案纳入Loss计算呢，比如每次从错误答案中随机 sample 1000个？你应该可以想到只要sample的数量合理，这样的方法可以达到与CELoss相近的效果，但是可以显著减小指数运算的次数，即不损失Performance的情况下加速Loss计算。理解了这一点你就理解了NCE的核心idea，NCE Loss可以是解决多分类问题的另一种工具，尤其是分类种类数很多的情况下

关于NCE为什么可以达到和CELoss相近的效果以及关于NCE更详尽的分析，请参考[这篇文章](https://leimao.github.io/article/Noise-Contrastive-Estimation/)

## Code Implementation

另外我在[这个仓库](https://github.com/Lingyu-Kong/Neural-Probabilistic-Language-Model)里面基于torch实现了RNN，LSTM以及CE，NCE。